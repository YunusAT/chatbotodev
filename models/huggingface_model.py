import os
import joblib
import pandas as pd
from dotenv import load_dotenv
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
from sklearn.metrics import classification_report, accuracy_score
from sentence_transformers import SentenceTransformer
from langchain_community.vectorstores import FAISS
from langchain_community.document_loaders import PyMuPDFLoader
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.chains import RetrievalQA
from langchain.prompts import PromptTemplate

# YEREL LLM Ä°Ã‡Ä°N YENÄ° Ä°MPORTLAR
from transformers import pipeline, AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig
from langchain_community.llms import HuggingFacePipeline
import torch  # GPU kontrolÃ¼ iÃ§in

# ğŸ“ Dosya yollarÄ±
BASE_DIR = os.path.dirname(os.path.abspath(__file__))

DATASET_PATH = os.path.join(BASE_DIR, "datas", "intent_dataset.xlsx")
PDF_PATH     = os.path.join(BASE_DIR, "datas", "depo_sorulari.pdf")
MODEL_DIR = os.path.join(BASE_DIR, "models")
MODEL_PATH   = os.path.join(MODEL_DIR, "intent_model.joblib")
ENCODER_PATH = os.path.join(MODEL_DIR, "encoder.joblib")
FAISS_PATH = os.path.join(BASE_DIR, "vectorstore", "faiss_index")

load_dotenv()

# ğŸ” Ortak embedding modeli
print("ğŸ”„ Embedding modeli yÃ¼kleniyor...")
embed_model = SentenceTransformer("sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")


# ğŸ”¹ FAISS Index OluÅŸturma
def create_faiss_index():
    print("ğŸ“„ PDF yÃ¼kleniyor...")
    loader = PyMuPDFLoader(PDF_PATH)
    documents = loader.load()
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    texts = splitter.split_documents(documents)
    print("ğŸ” FAISS index oluÅŸturuluyor...")
    # Embedding modeli burada da aynÄ± olmalÄ±
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    db = FAISS.from_documents(texts, embeddings)
    os.makedirs(os.path.dirname(FAISS_PATH), exist_ok=True)
    db.save_local(FAISS_PATH)
    print(f"âœ… FAISS index kaydedildi: {FAISS_PATH}")


# ğŸ”¹ Intent Model EÄŸitimi
def train_intent_classifier():
    df = pd.read_excel(DATASET_PATH).dropna()
    X, y = df["Ã¶rnek_cÃ¼mle"].tolist(), df["intent"].tolist()
    X_vec = embed_model.encode(X)
    le = LabelEncoder()
    y_encoded = le.fit_transform(y)
    X_train, X_test, y_train, y_test = train_test_split(X_vec, y_encoded, test_size=0.2, stratify=y_encoded,
                                                        random_state=42)
    clf = LogisticRegression(max_iter=1000)
    clf.fit(X_train, y_train)
    y_pred = clf.predict(X_test)
    print("ğŸ¯ DoÄŸruluk:", accuracy_score(y_test, y_pred))
    print("\nğŸ“Š Rapor:\n", classification_report(y_test, y_pred, target_names=le.classes_))
    os.makedirs(MODEL_DIR, exist_ok=True)
    joblib.dump(clf, MODEL_PATH)
    joblib.dump(le, ENCODER_PATH)
    print("âœ… Model ve encoder kaydedildi.")
    return clf, le


# ğŸ”¹ RAG Zinciri OluÅŸtur (Yerel bir Hugging Face modeli kullanÄ±lacak ÅŸekilde deÄŸiÅŸtirildi)
def create_qa_chain():
    embeddings = HuggingFaceEmbeddings(model_name="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2")
    vectorstore = FAISS.load_local(FAISS_PATH, embeddings, allow_dangerous_deserialization=True)

    model_id = "mistralai/Mistral-7B-Instruct-v0.2"  # Ã–rnek: Mistral 7B Instruct

    # GPU varlÄ±ÄŸÄ±nÄ± kontrol et ve cihazÄ± ayarla
    device = 0 if torch.cuda.is_available() else -1  # 0 GPU, -1 CPU
    print(f"LLM, cihaz Ã¼zerinde yÃ¼kleniyor: {'CUDA (GPU)' if device == 0 else 'CPU'}")

    try:
        # Nicemleme (Quantization) ayarlarÄ±: Bellek kullanÄ±mÄ±nÄ± azaltÄ±r.
        # Daha az belleÄŸe sahipseniz 'load_in_4bit=True' veya 'load_in_8bit=True' kullanÄ±n.
        # BÃ¼yÃ¼k modellerde bu zorunlu olabilir.
        bnb_config = BitsAndBytesConfig(
            load_in_4bit=True,  # 4-bit nicemleme
            bnb_4bit_quant_type="nf4",
            bnb_4bit_compute_dtype=torch.float16,
            bnb_4bit_use_double_quant=True,
        )

        # Modeli ve tokenleÅŸtiriciyi yÃ¼kle
        tokenizer = AutoTokenizer.from_pretrained(model_id)
        model = AutoModelForCausalLM.from_pretrained(
            model_id,
            quantization_config=bnb_config if torch.cuda.is_available() else None,  # GPU varsa nicemleme kullan
            device_map="auto" if torch.cuda.is_available() else None,  # GPU varsa otomatik cihaz eÅŸleme
            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
            # CPU Ã¼zerinde Ã§alÄ±ÅŸtÄ±rÄ±rken nicemlemeyi kapatÄ±n, Ã§Ã¼nkÃ¼ BitsAndBytes sadece GPU iÃ§indir.
        )
        model.eval()  # Modeli deÄŸerlendirme moduna al

        # Metin Ã¼retimi iÃ§in Hugging Face pipeline oluÅŸtur
        hf_pipeline = pipeline(
            "text-generation",
            model=model,
            tokenizer=tokenizer,
            # max_new_tokens: Ãœretilecek maksimum token sayÄ±sÄ±
            max_new_tokens=500,
            # do_sample: Rastgele Ã¶rnekleme yapÄ±lÄ±p yapÄ±lmayacaÄŸÄ± (True=daha yaratÄ±cÄ±, False=daha deterministik)
            do_sample=True,
            # temperature: Ã‡Ä±ktÄ±nÄ±n rastgeleliÄŸini kontrol eder (dÃ¼ÅŸÃ¼k=daha odaklÄ±, yÃ¼ksek=daha Ã§eÅŸitli)
            temperature=0.7,
            # top_k: En olasÄ± k token arasÄ±ndan seÃ§im yapar
            top_k=50,
            # top_p: Toplam olasÄ±lÄ±ÄŸÄ± p olan token grubundan seÃ§im yapar (Ã§ekirdek Ã¶rnekleme)
            top_p=0.95,
            # num_return_sequences: Ãœretilecek baÄŸÄ±msÄ±z sÄ±ra sayÄ±sÄ± (genellikle 1)
            num_return_sequences=1,
            # eos_token_id: CÃ¼mlenin sonu token ID'si, Ã¼retimi durdurmak iÃ§in
            eos_token_id=tokenizer.eos_token_id
        )

        llm = HuggingFacePipeline(pipeline=hf_pipeline)
        print(f"âœ… Yerel LLM '{model_id}' baÅŸarÄ±yla yÃ¼klendi.")

    except Exception as e:
        print(f"âŒ Yerel LLM yÃ¼klenirken bir hata oluÅŸtu: {e}")
        print("LÃ¼tfen yeterli bellek (RAM/VRAM) olduÄŸundan, gerekli kÃ¼tÃ¼phanelerin yÃ¼klÃ¼ olduÄŸundan")
        print("ve model ID'sinin doÄŸru olduÄŸundan emin olun. Daha kÃ¼Ã§Ã¼k bir model deneyebilirsiniz.")
        return None  # Hata durumunda None dÃ¶ndÃ¼r

    # --- YEREL LLM ENTEGRASYONU SONU ---

    prompt = PromptTemplate(
        template="""Sen lojistik destek konusunda uzman bir yardÄ±mcÄ±sÄ±n.
KullanÄ±cÄ± ÅŸu soruyu sordu: "{question}"
AÅŸaÄŸÄ±daki belgeleri oku ve net, kÄ±sa ve profesyonel bir yanÄ±t ver:

{context}
YanÄ±t:""",  # YanÄ±tÄ±n baÅŸlangÄ±cÄ±nÄ± belirtmek iÃ§in "YanÄ±t:" ekledim
        input_variables=["question", "context"]
    )
    return RetrievalQA.from_chain_type(
        llm=llm,
        retriever=vectorstore.as_retriever(),
        chain_type_kwargs={"prompt": prompt},
        return_source_documents=False
    )


# ğŸ”¹ Chatbot DÃ¶ngÃ¼sÃ¼
def chatbot_loop(clf, le, qa_chain):
    print("\nğŸ’¬ Lojistik Destek Chatbotu (Ã§Ä±kmak iÃ§in 'Ã§Ä±k', 'exit' veya 'q' yazÄ±n)\n")
    if qa_chain is None:
        print("Chatbot baÅŸlatÄ±lamadÄ± Ã§Ã¼nkÃ¼ LLM yÃ¼klenirken bir sorun oluÅŸtu.")
        return

    while True:
        user_input = input("Siz: ")
        if user_input.lower() in ["Ã§Ä±k", "exit", "q"]:
            print("ğŸ‘‹ GÃ¶rÃ¼ÅŸmek Ã¼zere!")
            break

        # KullanÄ±cÄ± girdisini vektÃ¶rleÅŸtir ve niyeti tahmin et
        vec = embed_model.encode([user_input])
        intent = le.inverse_transform(clf.predict(vec))[0]

        print(f"Tahmin edilen niyet: {intent}")  # Niyetin ne olduÄŸunu gÃ¶rmek iÃ§in

        if intent == "greeting":
            print("Bot: Merhaba! Size nasÄ±l yardÄ±mcÄ± olabilirim?")
        elif intent == "goodbye":
            print("Bot: GÃ¶rÃ¼ÅŸmek Ã¼zere!")
        elif intent == "konu_dÄ±ÅŸÄ±":
            print("Bot: Bu konuda yardÄ±mcÄ± olamÄ±yorum. LÃ¼tfen lojistikle ilgili bir ÅŸey sorun.")
        else:
            print("ğŸ” Bilgi aranÄ±yor...")
            try:
                # QA zincirini Ã§alÄ±ÅŸtÄ±r
                response = qa_chain.run(user_input)
                # Model Ã§Ä±ktÄ±sÄ±nda oluÅŸan potansiyel istem tekrarÄ±nÄ± temizle
                if response.startswith(user_input):
                    response = response[len(user_input):].strip()
                # Bazen model "YanÄ±t:" gibi istenmeyen metinler Ã¼retebilir, onlarÄ± temizle
                if response.startswith("YanÄ±t:"):
                    response = response[len("YanÄ±t:"):].strip()
                print("Bot:", response.strip())  # Fazla boÅŸluklarÄ± temizle
            except Exception as e:
                print(f"Bot: YanÄ±t oluÅŸturulurken bir hata oluÅŸtu: {e}")
                print("LÃ¼tfen LLM'in doÄŸru yÃ¼klendiÄŸinden ve Ã§alÄ±ÅŸtÄ±ÄŸÄ±ndan emin olun.")


# ğŸ”¹ Ana AkÄ±ÅŸ
if __name__ == "__main__":
    create_faiss_index()  # FAISS indeksini oluÅŸtur
    clf, le = train_intent_classifier()  # Niyet sÄ±nÄ±flandÄ±rÄ±cÄ±yÄ± eÄŸit
    qa_chain = create_qa_chain()  # RAG zincirini oluÅŸtur (yerel LLM ile)
    chatbot_loop(clf, le, qa_chain)  # Chatbot dÃ¶ngÃ¼sÃ¼nÃ¼ baÅŸlat